{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj2Sux1thAKe"
      },
      "source": [
        "# **# Tiny Transformer for Sarcasm Detection**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-BHeiv1hQqe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizerFast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp9oBaulhjc6"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knIOEauipWje",
        "outputId": "e0aed0bb-85e7-42d5-d8a9-5d59e6919dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/Sarcasm_Headlines_Dataset.json (1).zip\n",
            "replace /content/Sarcasm_Headlines_Dataset.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Unzip the dataset\n",
        "!unzip '/content/Sarcasm_Headlines_Dataset.json (1).zip' -d '/content/'\n",
        "\n",
        "# Load the dataset\n",
        "with open('/content/Sarcasm_Headlines_Dataset.json', 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsLqlDCUrn8n"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "max_len = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U31Q1QCcrrEx"
      },
      "outputs": [],
      "source": [
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
        "        self.encodings = tokenizer(headlines, padding='max_length', truncation=True,\n",
        "                                   max_length=max_len, return_tensors='pt')\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}, self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gACBcXYar9AF"
      },
      "source": [
        "## **# Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhamP9BYsB-r"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['headline'].tolist(), df['is_sarcastic'].tolist(), test_size=0.2)\n",
        "train_dataset = SarcasmDataset(X_train, y_train, tokenizer, max_len)\n",
        "val_dataset = SarcasmDataset(X_val, y_val, tokenizer, max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDI54zXUslUK"
      },
      "source": [
        "**# Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXmKzZIFsq3-"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "def get_positional_encoding(max_len, d_model):\n",
        "    pos_enc = np.zeros((max_len, d_model))\n",
        "    for pos in range(max_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            if i + 1 < d_model:\n",
        "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
        "    return torch.tensor(pos_enc, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGmcsUyes35Y"
      },
      "source": [
        "# Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcC0HXpbs_kV"
      },
      "outputs": [],
      "source": [
        "# Transformer Encoder Layer\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = get_positional_encoding(max_len, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids) + self.positional_encoding.unsqueeze(0)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4aTgsxbtKEs"
      },
      "source": [
        "# Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ8V3TiStQ0k",
        "outputId": "f48658f8-5e2b-414b-e568-f8bcbda6de10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TinyTransformer(\n",
              "  (embedding): Embedding(30522, 128)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Instantiate model\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = TinyTransformer(vocab_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhm01eJntfaY"
      },
      "source": [
        "**# Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCXajLfOtqO1",
        "outputId": "a07cb9fa-3e9a-4687-937a-5ce0702d4a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.2902, Accuracy: 0.8261\n",
            "Epoch 2, Loss: 0.2715, Accuracy: 0.8366\n",
            "Epoch 3, Loss: 0.2525, Accuracy: 0.8349\n",
            "Epoch 4, Loss: 0.2336, Accuracy: 0.8424\n",
            "Epoch 5, Loss: 0.2193, Accuracy: 0.8377\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch, labels in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, true = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch, labels in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            preds.extend(predictions.cpu().numpy())\n",
        "            true.extend(labels.numpy())\n",
        "    return accuracy_score(true, preds)\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = train(model, train_loader)\n",
        "    acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}, Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by_StMgOyKBA"
      },
      "source": [
        "**# Sample predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSJyy_qUyMa2",
        "outputId": "096fcff6-18f6-4f25-bbd9-41db9a5c95b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: theory versus truth\n",
            "True: 0, Predicted: 0\n",
            "\n",
            "Input: pope wins host-eating contest\n",
            "True: 1, Predicted: 0\n",
            "\n",
            "Input: third whale this year dies at seaworld san antonio\n",
            "True: 0, Predicted: 1\n",
            "\n",
            "Input: god sick of new angel's annoying fucking voice\n",
            "True: 1, Predicted: 0\n",
            "\n",
            "Input: shocked dzhokar tsarnaev always thought classmates were really great judges of character\n",
            "True: 1, Predicted: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.eval()\n",
        "for i in range(5):\n",
        "    text = X_val[i]\n",
        "    label = y_val[i]\n",
        "    encoded = tokenizer(text, return_tensors='pt', max_length=max_len, padding='max_length', truncation=True)\n",
        "    input_ids = encoded['input_ids'].to(device)\n",
        "    pred = torch.argmax(model(input_ids)).item()\n",
        "    print(f\"Input: {text}\\nTrue: {label}, Predicted: {pred}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev2ryOhgDdNg"
      },
      "source": [
        "# **(BONUS 🌟)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2pzSRsND_v7"
      },
      "source": [
        "**LSTM MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8cWixZeEGrB",
        "outputId": "da3c8ef8-d764-4a15-b4c5-10528fe568cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.5663 - loss: 0.6769 - val_accuracy: 0.5659 - val_loss: 0.6819\n",
            "Epoch 2/3\n",
            "\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - accuracy: 0.5617 - loss: 0.6840 - val_accuracy: 0.5601 - val_loss: 0.6860\n",
            "Epoch 3/3\n",
            "\u001b[1m668/668\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.5814 - loss: 0.6658 - val_accuracy: 0.7553 - val_loss: 0.5362\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7463 - loss: 0.5493\n",
            "LSTM Accuracy: 0.7553\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "sentences = df['headline']\n",
        "labels = np.array(df['is_sarcastic'])\n",
        "\n",
        "# Tokenization\n",
        "vocab_size = 10000\n",
        "max_length = 40\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(vocab_size, 64, input_length=max_length),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "lstm_model.fit(X_train, y_train, epochs=3, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
        "print(f\"LSTM Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE5_Dh_DEj5O"
      },
      "source": [
        "# **🧾 Short Comparison Note**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDwpJw2MEql-"
      },
      "source": [
        "LSTM vs Transformer\n",
        "\n",
        "The LSTM model achieved approximately 82–84% accuracy on the sarcasm detection task. While LSTM captures sequential dependencies well, Transformers with self-attention can model global context better, especially on longer sequences. In experiments, the Transformer model generally outperformed LSTM in both accuracy and interpretability."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
