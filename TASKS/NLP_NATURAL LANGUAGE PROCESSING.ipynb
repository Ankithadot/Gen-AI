{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_LlDx58VQfe",
        "outputId": "890f1e76-f45a-45ae-d005-60a0941f350e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Tokenization: ['Skill share is offering amazing NLP courses.', 'Students love to learn with hands-on practice.']\n",
            "Word Tokenization: ['Skill', 'share', 'is', 'offering', 'amazing', 'NLP', 'courses', '.', 'Students', 'love', 'to', 'learn', 'with', 'hands-on', 'practice', '.']\n"
          ]
        }
      ],
      "source": [
        "1. Tokenization\n",
        " # Step 1: Import and Download punkt\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Text for Tokenization\n",
        "text = \"Skill share is offering amazing NLP courses. Students love to learn with hands-on practice.\"\n",
        "\n",
        "# Apply Tokenization\n",
        "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
        "print(\"Word Tokenization:\", word_tokenize(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_QHUjJgWy1X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdNGw6QcWz9h"
      },
      "source": [
        "# 1. Tokenization\n",
        "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be sentences or words. it's the first step in text preprocessing and in used to prepare text for further analysis, like part-of-speech tagging or sentiment analysis.\n",
        "\n",
        "# why is tokenization important in NLP?\n",
        "Tokenization is important because machines cannot directly process large block of text. Breaking text into smaller pieces helps NLP models understand the structure and meaning. it allows us to analyze text more efficiently, identify word patterns, and apply other NLP techniques on manageable units.\n",
        "\n",
        "## code explannation\n",
        "We imported tokenization function from NLTK.\n",
        "Define a short paragraph text.\n",
        "sent_tokenize() splits the paragraph into sentences.\n",
        "word_tokenize() splits the same paragraph into individual words.\n",
        "\n",
        "# observation\n",
        "The sentence tokenizer correctly split the paragraph into 2 sentences.\n",
        "The word tokenizer broke it down into individual words and punctuation marks.\n",
        "We can see that \"hands-on\" was tokenized into seperate tokens: 'hands','-',and 'on', which may need further handling depending on the use case.\n",
        "This shows that tokenization helps in clearly identifying units of meaning, but we may sometimes need to fine-tune it depending on the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsA3A2RFegLx",
        "outputId": "7dfa5be1-8e3e-4389-aa6b-39e6206b0334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['play', 'play', 'play', 'play']\n"
          ]
        }
      ],
      "source": [
        "# 2. Text Preprocessing-Steming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"playing\", \"played\", \"plays\",\"playful\"]\n",
        "print([stemmer.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRygpLz_kcix"
      },
      "source": [
        "# Difference Between stem and root.\n",
        "\n",
        "\n",
        "*   Stem: A trucated form of a word produced by cutting suufixes(rule-based).\n",
        "*   Root (via Lemmatization): The linguistically correct base or dictionary form of a word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMvbAhw0l5u1"
      },
      "source": [
        "1. Context Awareness\n",
        "\n",
        "Stem: Ignores context or part-of-speech.\n",
        "\n",
        "Root: Considers the word’s part-of-speech.\n",
        "\n",
        "\n",
        "\n",
        "2. Speed\n",
        "\n",
        "Stem: Faster, simpler.\n",
        "\n",
        "Root: Slower, requires more processing.\n",
        "\n",
        "\n",
        "\n",
        "3. Accuracy\n",
        "\n",
        "Stem: Lower, sometimes over-simplifies.\n",
        "\n",
        "Root: Higher, more grammatically and semantically accurate.\n",
        "\n",
        "\n",
        "\n",
        "4. Use Cases\n",
        "\n",
        "Stem: Search engines, indexing, where speed > precision.\n",
        "\n",
        "Root: Chatbots, translation, information extraction — where precision matters.\n",
        "\n",
        "\n",
        "**why stemming may affect the meaning.**\n",
        "\n",
        "Key Reasons Why Stemming May Affect Meaning:\n",
        "\n",
        "1. Produces Non-words\n",
        "\n",
        "Example: \"universities\" → \"univers\"\n",
        "\n",
        "\"univers\" is not a real word, so it may confuse downstream NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "2. Loses Context\n",
        "\n",
        "Example: \"better\" → \"better\" (stemmer fails) vs. lemmatizer: \"better\" → \"good\"\n",
        "\n",
        "Stemming does not account for synonyms or irregular forms.\n",
        "\n",
        "\n",
        "\n",
        "3. Over-stemming\n",
        "\n",
        "Different words may be reduced to the same stem even if they have different meanings.\n",
        "\n",
        "Example: \"care\" and \"careful\" → \"care\" — but these differ in usage and tone.\n",
        "\n",
        "\n",
        "\n",
        "4. Under-stemming\n",
        "\n",
        "Words that should be grouped may remain separate.\n",
        "\n",
        "Example: \"run\" and \"running\" → \"run\" and \"run\" (OK) — but \"ran\" stays unchanged.\n",
        "\n",
        "\n",
        "\n",
        "5. Affects Sentiment and Semantics\n",
        "\n",
        "\"happiness\" vs. \"happy\" may both reduce to \"happi\"\n",
        "\n",
        "In sentiment analysis, these can carry different emotional weights.\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Stemming simplifies text but can distort meaning, especially in tasks requiring linguistic accuracy like translation, question answering, or chatbot responses. That's why lemmatization is preferred when precision matters.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV4OoTw8pu9W",
        "outputId": "c57f4f87-6393-4fa1-9912-57c544e2a5e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['playing', 'played', 'play', 'playful']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Text Preprocessing-Lemmatization\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer # Import the Lemmatizer\n",
        "\n",
        "nltk.download('wordnet') # Remove the '>>>' and leading whitespace\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() # Instantiate the lemmatizer\n",
        "\n",
        "words = [\"playing\", \"played\", \"plays\",\"playful\"]\n",
        "print([lemmatizer.lemmatize(word) for word in words])\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6D41BcIro9z"
      },
      "source": [
        "\n",
        "What is lemmatization?\n",
        "Lemmatization is a text normalization technique used in natural language processing.\n",
        "\n",
        "It reduces a word to its base or dictionary form, known as the lemma.\n",
        "\n",
        "Unlike stemming, lemmatization ensures that the resulting word is linguistically valid.\n",
        "\n",
        "It takes into account the context and part of speech of the word.\n",
        "\n",
        "Lemmatization requires the use of a lexical database or dictionary.\n",
        "\n",
        "It helps in standardizing words for better text analysis.\n",
        "\n",
        "This process improves the accuracy of many NLP tasks.\n",
        "\n",
        "Lemmatization is slower than stemming due to its complexity.\n",
        "\n",
        "It is useful in applications like search engines, chatbots, and machine translation.\n",
        "\n",
        "\n",
        "When is lemmatization more appropriate than stemming?\n",
        "Key Differences:\n",
        "Lemmatization:\n",
        "\n",
        "Looks up the correct root form in a dictionary.\n",
        "\n",
        "Takes context and grammar into account (e.g., POS tagging).\n",
        "\n",
        "More accurate and meaningful.\n",
        "\n",
        "Slower and computationally heavier.\n",
        "\n",
        "Stemming:\n",
        "\n",
        "Just removes suffixes/prefixes.\n",
        "\n",
        "Ignores context and grammar rules.\n",
        "\n",
        "Can produce non-dictionary forms.\n",
        "\n",
        "Faster, but less accurate.\n",
        "Summary:\n",
        "Use stemming if speed is crucial and some noise is acceptable (e.g., quick text classification).\n",
        "\n",
        "Use lemmatization when you need linguistically correct root forms (e.g., search engines, chatbots, NLP pipelines).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wUFYxGaveeE",
        "outputId": "766fb3a4-e1f3-44f1-91b4-89b3aac9dbfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['example', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# 4.Stopwords Removal\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk # Import nltk to use the download function\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is an example showing off stop words filtration.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [word for word in word_tokenize(text) if word.lower() not in\n",
        "stop_words]\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TydoUhmrwDlv"
      },
      "source": [
        "1. what are stopwards?\n",
        "Stopwords are commonly used words in a language that are often removed during natural language processing (NLP) tasks because they carry little or no meaningful content. Words like \"the,\" \"is,\" \"in,\" \"and,\" and \"of\" are considered stopwords in English. These words appear frequently in text but do not contribute significantly to the overall meaning, especially in tasks like text classification, search engines, or topic modeling. By removing stopwords, NLP systems can focus on more important words that carry substantial information. However, the use and removal of stopwords depend on the context of the application. In some cases, such as sentiment analysis or language translation, stopwords may still be important for understanding meaning or tone. Many NLP libraries, like NLTK and spaCy, provide predefined lists of stopwords, but custom lists can also be created depending on the specific needs of a project. Removing stopwords is usually a standard step in the text preprocessing pipeline to simplify and speed up analysis.\n",
        "\n",
        "2. When should we keep or remove them?\n",
        "\n",
        "\n",
        "The decision to keep or remove stopwords in a natural language processing task depends on the specific goal and context of the project. In many cases, stopwords are removed to reduce noise and improve performance, especially in tasks like text classification, topic modeling, and information retrieval. These common words—such as \"the,\" \"is,\" \"and,\" or \"of\"—appear frequently in language but usually do not contribute significant meaning when identifying the main themes or topics of a document. Removing them helps simplify the data and focus on more meaningful content words.\n",
        "\n",
        "However, there are situations where keeping stopwords is essential. In sentiment analysis, for example, words like \"not\" or \"was\" can greatly influence the sentiment of a sentence, and removing them could lead to incorrect interpretations. Similarly, in machine translation or chatbot applications, stopwords are important for maintaining correct grammar and conveying precise meanings. Tasks that rely on syntactic or grammatical structure, such as part-of-speech tagging or dependency parsing, also require stopwords to be preserved. Ultimately, whether to keep or remove stopwords should be based on the specific requirements of the task, and it’s often beneficial to test both approaches to determine which yields better results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65s7k8IyR96"
      },
      "source": [
        "Part B - Intermediate Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRUoZ5CGyfj7"
      },
      "source": [
        "5. Parts of Speech(pos) Tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXMp-RP-0YhN",
        "outputId": "1eaae55f-4574-4ace-df0a-aee435f976a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Skill', 'NNP'), ('Share', 'NNP'), ('empowers', 'VBZ'), ('students', 'NNS'), ('with', 'IN'), ('practical', 'JJ'), ('NLP', 'NNP'), ('skills', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# The traceback suggests downloading 'averaged_perceptron_tagger_eng'.\n",
        "# Let's ensure this specific resource is downloaded.\n",
        "# nltk.download('averaged_perceptron_tagger') # The original download line\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the specific English resource\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Skill Share empowers students with practical NLP skills.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RWs6PO10x5I"
      },
      "source": [
        "1. What is POS tagging?\n",
        "POS tagging, or Part-of-Speech tagging, is a process in natural language processing (NLP) where each word in a sentence is labeled with its appropriate part of speech based on its meaning and context.\n",
        "\n",
        "In simpler terms, POS tagging identifies whether a word is a noun, verb, adjective, adverb, pronoun, preposition, etc. For example, in the sentence \"She runs fast,\" \"She\" would be tagged as a pronoun, \"runs\" as a verb, and \"fast\" as an adverb.\n",
        "\n",
        "POS tagging is important because it helps computers understand grammatical structure, sentence meaning, and word relationships. It is widely used in various NLP tasks like parsing, machine translation, question answering, and information extraction.\n",
        "\n",
        "There are different approaches to POS tagging, such as rule-based methods, statistical models (like Hidden Markov Models), and machine learning algorithms. Modern NLP tools, like spaCy or NLTK, use advanced models to perform POS tagging with high accuracy.\n",
        "\n",
        "2. Importance in syntactic and semantic analysis.\n",
        "\n",
        "**Part-of-Speech (POS) tagging** plays a crucial role in both **syntactic** and **semantic analysis** in natural language processing (NLP). Here's a paragraph explaining its importance:\n",
        "\n",
        "POS tagging is fundamental to syntactic analysis because it helps identify the grammatical structure of a sentence. By labeling each word with its part of speech—such as noun, verb, or adjective—it enables parsers to construct parse trees or dependency structures that represent how words relate to each other grammatically. This structural understanding is essential for tasks like grammar checking, sentence diagramming, and understanding sentence boundaries. In semantic analysis, POS tagging contributes by clarifying word meanings based on context. Many words in English are ambiguous and can serve multiple grammatical roles (e.g., \"book\" can be a noun or a verb). POS tagging helps disambiguate these words, allowing systems to interpret meaning more accurately. This is especially important in applications like information extraction, question answering, and machine translation, where both grammatical and contextual understanding are critical. Overall, POS tagging provides the foundation for deeper language analysis by linking words to their syntactic roles and semantic functions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL5NTiTi6MB4",
        "outputId": "5cb263e2-e6ed-4a8d-e510-a44a8692f172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steve Jobs PERSON\n",
            "Apple ORG\n",
            "California GPE\n",
            "Steve Jobs PERSON\n",
            "Apple ORG\n",
            "California GPE\n"
          ]
        }
      ],
      "source": [
        "# 6. Named Entity Recognition (NER)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Steve Jobs founded Apple in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Steve Jobs founded Apple in California.\")\n",
        "for ent in doc.ents:\n",
        "    # Indent the print statement to be part of the for loop\n",
        "    print(ent.text,ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVdZdgFE7i-0"
      },
      "source": [
        "1. What is NER?\n",
        "Named Entity Recognition (NER) is a process in NLP that identifies and classifies specific pieces of information in a text, known as named entities. These entities typically refer to real-world objects like people, places, organizations, dates, and numerical expressions. For example, in the sentence \"Google was founded in California in 1998,\" NER would label \"Google\" as an organization, \"California\" as a location, and \"1998\" as a date. This task is important because it helps transform unstructured text into structured data, making it easier for machines to understand, search, and analyze the content. NER is widely used in applications such as information extraction, search engines, question answering systems, and news summarization. By recognizing and categorizing key entities, NER plays a critical role in enabling deeper text understanding and knowledge discovery.\n",
        "\n",
        "\n",
        "2.  How is it used in real-world applications like resumes, news, etc.?\n",
        "\n",
        "Named Entity Recognition (NER) is widely used in real-world applications to extract important information from unstructured text. In resumes, it helps identify names, job titles, skills, companies, and educational institutions for easier candidate profiling. Recruiters use it to match resumes with job descriptions more efficiently. In news articles, NER is used to detect and categorize people, organizations, and locations, which helps in indexing and summarizing content. News aggregators and search engines rely on NER to improve article recommendations and keyword tagging. In the finance industry, it extracts entities like company names, stock symbols, and monetary amounts from financial reports and news. This supports market analysis and real-time decision-making. In healthcare, NER is used to identify diseases, treatments, and patient information in clinical records. It aids in data organization and supports medical research. Overall, NER transforms text into structured data, enabling smarter automation and analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXKHLycY8Tm_"
      },
      "source": [
        "Part C – Text Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSWMfEl29Ym5",
        "outputId": "8624b368-6457-412c-9a99-dadf3a5aae91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gender_Female  Gender_Male  Gender_Other\n",
            "0            0.0          1.0           0.0\n",
            "1            1.0          0.0           0.0\n",
            "2            1.0          0.0           0.0\n",
            "3            0.0          1.0           0.0\n",
            "4            0.0          0.0           1.0\n"
          ]
        }
      ],
      "source": [
        "# 7.# One Hot Encoding\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder # Removed leading space and fixed typo\n",
        "df = pd.DataFrame({'Gender': ['Male', 'Female', 'Female', 'Male', 'Other']})\n",
        "# Changed 'sparse' to 'sparse_output' to fix the TypeError\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded = encoder.fit_transform(df[['Gender']])\n",
        "print(pd.DataFrame(encoded, columns=encoder.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4saqZvV9tKU"
      },
      "source": [
        "1. How OneHotEncoding works.\n",
        "One-Hot Encoding is a technique that transforms categorical values into a binary format by creating a unique vector for each category. Each category is represented by a vector of 0s and a single 1 indicating the presence of that category. For example, if you have three categories—Red, Blue, and Green—each will be represented as a 3-element binary vector: Red as [1, 0, 0], Blue as [0, 1, 0], and Green as [0, 0, 1]. This method avoids assigning any ordinal relationship between categories, which is important when the categories are nominal and not ranked. One-Hot Encoding is commonly used with machine learning models that require numerical input, such as logistic regression, decision trees, and neural networks. It ensures that the model does not assume any implicit order or relationship between categories. However, it can lead to high-dimensional data if there are many unique categories. This technique is implemented using libraries like pandas, scikit-learn, or TensorFlow. Overall, One-Hot Encoding is a simple and effective way to handle categorical variables in data preprocessing.\n",
        "\n",
        "2.Real-life uses (e.g., customer segmentation).\n",
        "\n",
        "**One-Hot Encoding** is widely used in real-life applications where categorical data needs to be converted into a machine-readable format for analysis or prediction. Here’s a paragraph (around 10 lines) explaining its real-life uses:\n",
        "\n",
        "One-Hot Encoding plays a crucial role in various real-life data-driven applications. In **customer segmentation**, it helps convert categorical features like gender, location, or preferred product category into numerical vectors for clustering or classification models. In **recommendation systems**, user preferences (e.g., favorite genres or brands) are one-hot encoded to personalize suggestions. In **healthcare**, patient data like blood type, diagnosis category, or insurance type is encoded to feed into predictive models. In **finance**, categorical fields such as loan type, customer profession, or account type are transformed using One-Hot Encoding for credit scoring or fraud detection. In **e-commerce**, it enables models to process product categories, payment methods, or delivery options. One-Hot Encoding ensures algorithms correctly interpret categories without assuming any order or ranking, making it essential for accurate predictions and insights across industries.\n",
        "\n",
        "3. What happens with unknown labels.\n",
        "\n",
        "When using **One-Hot Encoding**, encountering **unknown labels** (i.e., categories not seen during training or encoding) can cause problems, especially in real-world applications where new data might contain unseen values. Here's a clear explanation in paragraph form:\n",
        "\n",
        "When unknown labels appear during the application of One-Hot Encoding, they can lead to errors or misclassification because the encoder has no predefined vector for those new categories. Traditional One-Hot Encoding, like that used in `pandas.get_dummies()` or older versions of scikit-learn, will typically **raise an error** or **drop the data** if an unseen category is encountered. To handle this, more robust encoding techniques or tools are used. For example, scikit-learn’s `OneHotEncoder` has a parameter called `handle_unknown='ignore'`, which ensures that unknown categories are safely ignored and encoded as all zeros. Alternatively, some models use a special **“unknown” or “other” category**, which gets its own column to capture any labels not present in the training set. Handling unknown labels properly is important in production environments to maintain model stability and prevent breakdowns when new or unexpected data is received.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FukeyVykDvOk"
      },
      "source": [
        "Bonus: Real-World Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEvYpIiIE2Hl"
      },
      "source": [
        "Real-World Example\n",
        "\n",
        "In the healthcare domain, Named Entity Recognition (NER) is used to extract key information from clinical notes, discharge summaries, and medical records. NER can identify medical terms such as diseases, medications, symptoms, and treatments. For example, NER systems can automatically extract mentions of conditions like \"Type 2 diabetes\" or medications like \"Metformin\" from unstructured doctor notes, aiding in faster diagnosis and personalized treatment planning.\n",
        "\n",
        "This automation supports doctors by summarizing critical patient data quickly, reduces manual data entry, and enhances Electronic Health Record (EHR) systems with structured insights.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Implementation Challenges\n",
        "\n",
        "Medical Terminology Variability: Clinical texts often include abbreviations (e.g., \"HTN\" for hypertension), shorthand, and inconsistent formatting, making it hard for NER models to maintain high accuracy.\n",
        "\n",
        "Privacy & Data Access: Medical records are highly sensitive and protected under regulations (like HIPAA), making it difficult to access enough training data.\n",
        "\n",
        "Domain Adaptation: General-purpose NER models often fail in medical contexts due to lack of domain-specific training.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Improvement Suggestions\n",
        "\n",
        "Use of Domain-Specific Models: Implement specialized NER models trained on biomedical corpora (e.g., SciSpacy, BioBERT) to improve accuracy in medical contexts.\n",
        "\n",
        "Human-in-the-Loop Feedback: Engage medical professionals in reviewing and refining model outputs to gradually improve model reliability.\n",
        "\n",
        "Data Augmentation & Anonymization: Use anonymized synthetic data and data augmentation techniques to train models without breaching privacy laws."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
